{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sklearn/문장 특징 추출과 유사도 측정\n",
    "\n",
    "https://blog.breezymind.com/2018/03/02/sklearn-feature_extraction-text-2/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## : Scikit-Learn의 text 특징추출과 벡터화를 통한 유사도 측정(CountVecotrizer, TfidVectorizer, HashingVectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문장 특징추출 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "import numpy as np\n",
    "np.random.seed(0) # 난수 발생 시드 설정 --> 무엇을 의미?\n",
    "from konlpy.tag import *\n",
    "okt = Okt()\n",
    "han = Hannanum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel, cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer : 문장에서 색인어 추출을 위해 명사, 동사, 알파벳, 숫자 정도의 단어만 뽑아서 normalization, stemming 처리하도록 함\n",
    "def tokenizer(raw, pos=[\"Noun\", \"Alpha\", \"Verb\", \"Number\"], storpword=[]):\n",
    "    return [\n",
    "        word for word, tag in han.pos(\n",
    "        raw\n",
    "        #normalize=True,\n",
    "        #stemming=True\n",
    "        )\n",
    "        if len(word) > 1 and tag in pos and word not in stopword\n",
    "    ]\n",
    "\n",
    "\n",
    "#테스트 문장\n",
    "rawdata = [\n",
    "    '아시아의 고산지대인 파미르, 텐샨,히말라야 의 자연과 사람들의 삶을 알아본다.',\n",
    "    '중앙아시아의 거대한 호수 ‘아랄해’, 지금은 거의 말라버려 20세기 최악의 환경재앙이라 불리는 곳입니다.',\n",
    "    '2부는 이 아랄해로 들어오는 두 물줄기 ‘아무다리아강’과 ‘시르다리아강’을 따라 그 발원지를 찾아가는 여정입니다.',\n",
    "    '<주요 에피소드> 아무다리아강의 발원지, 파미르고원 대륙빙하 중 최대 규모인 파미르의 페드첸코 빙하에서부터 녹아 흐르는 물은 판즈강을 거쳐 아무다리아 강이 된다.',\n",
    "    '타지키스탄과 아프가니스탄의 국경을 이루는 강인 판즈강을 따라 가면 쉽게 볼 수 없는 아프가니스탄 사람들의 모습을 볼 수 있다.',\n",
    "    ' 또 빙하부터 습지대까지 식생의 변화와, 환경의 변화를 살펴보고 강에 기대어 살아가는 사람들의 삶을 소개한다.   시르다리아강의 발원지는 텐샨산맥 파미르 고원에 비해 고도가 낮은 텐샨산맥은 아름다운 풍경을 자랑한다.',\n",
    "    ' 초록으로 뒤덮인 산과 색색의 꽃들은 화려한 텐샨을 만든다. 시르다리야강의 발원지인 텐샨의 빙하는 어떤 모습인지 소개한다. 자일로(여름 초목지)에 유르트를 짓고 양을 치는 키르기 유목민, 텐샨산맥의 야생동물을 만난다. '\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-3cac53434218>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# 문장에서 노출되는 feature(특징이 될만한 단어) 수를 합한 Document Term Matrix(이하 DTM) 을 리턴한다\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorize\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrawdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m print(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m    867\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    868\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[1;32m--> 869\u001b[1;33m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[0;32m    870\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    871\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m    809\u001b[0m             \u001b[0mvocabulary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m                 raise ValueError(\"empty vocabulary; perhaps the documents only\"\n\u001b[0m\u001b[0;32m    812\u001b[0m                                  \" contain stop words\")\n\u001b[0;32m    813\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "vectorize = CountVectorizer(\n",
    "    tokenizer=tokenizer, \n",
    "    min_df=2    # 예제로 보기 좋게 1번 정도만 노출되는 단어들은 무시하기로 했다\n",
    "                # min_df = 0.01 : 문서의 1% 미만으로 나타나는 단어 무시\n",
    "                # min_df = 10 : 문서에 10개 미만으로 나타나는 단어 무시\n",
    "                # max_df = 0.80 : 문서의 80% 이상에 나타나는 단어 무시\n",
    "                # max_df = 10 : 10개 이상의 문서에 나타나는 단어 무시\n",
    ")\n",
    " \n",
    "# 문장에서 노출되는 feature(특징이 될만한 단어) 수를 합한 Document Term Matrix(이하 DTM) 을 리턴한다\n",
    "X = vectorize.fit_transform(rawdata)\n",
    " \n",
    "print(\n",
    "    'fit_transform, (sentence {}, feature {})'.format(X.shape[0], X.shape[1])\n",
    ")\n",
    "# fit_transform, (sentence 5, feature 7)\n",
    " \n",
    "print(type(X))\n",
    "# <class 'scipy.sparse.csr.csr_matrix'>\n",
    " \n",
    "print(X.toarray())\n",
    " \n",
    "# [[0, 1, 2, 0, 0, 0, 1],\n",
    "# [0, 1, 1, 0, 0, 0, 2],\n",
    "# [1, 0, 0, 2, 1, 1, 0],\n",
    "# [1, 0, 0, 1, 0, 0, 0],\n",
    "# [0, 0, 0, 3, 1, 1, 0]]\n",
    " \n",
    "# 문장에서 뽑아낸 feature 들의 배열\n",
    "features = vectorize.get_feature_names()\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dayoi\\Anaconda3\\lib\\site-packages\\konlpy\\tag\\_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    " \n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    " \n",
    "from konlpy.tag import Twitter\n",
    "twitter = Twitter()\n",
    " \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel, cosine_similarity\n",
    " \n",
    "# tokenizer : 문장에서 색인어 추출을 위해 명사,동사,알파벳,숫자 정도의 단어만 뽑아서 normalization, stemming 처리하도록 함\n",
    "def tokenizer(raw, pos=[\"Noun\",\"Alpha\",\"Verb\",\"Number\"], stopword=[]):\n",
    "    return [\n",
    "        word for word, tag in twitter.pos(\n",
    "            raw, \n",
    "            norm=True,   # normalize 그랰ㅋㅋ -> 그래ㅋㅋ\n",
    "            stem=True    # stemming 바뀌나->바뀌다\n",
    "            )\n",
    "            if len(word) > 1 and tag in pos and word not in stopword\n",
    "        ]\n",
    "\n",
    "# 테스트 문장\n",
    "rawdata = [\n",
    "    '남북 고위급회담 대표단 확정..남북 해빙모드 급물살',\n",
    "    '[남북 고위급 회담]장차관만 6명..판 커지는 올림픽 회담',\n",
    "    \n",
    "    '문재인 대통령과 대통령의 영부인 김정숙여사 내외의 동반 1987 관람 후 인터뷰',\n",
    "    '1987 본 문 대통령..\"그런다고 바뀌나? 함께 하면 바뀐다\"',\n",
    "    \n",
    "    '이명박 전 대통령과 전 대통령의 부인 김윤옥 여사, 그리고 전 대통령의 아들 이시형씨의 동반 검찰출석이 기대됨'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit_transform, (sentence 5, feature 7)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "[[0 1 2 0 0 0 1]\n",
      " [0 1 1 0 0 0 2]\n",
      " [1 0 0 2 1 1 0]\n",
      " [1 0 0 1 0 0 0]\n",
      " [0 0 0 3 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "vectorize = CountVectorizer(\n",
    "tokenizer=tokenizer, \n",
    "    min_df=2    # 예제로 보기 좋게 1번 정도만 노출되는 단어들은 무시하기로 했다\n",
    "                # min_df = 0.01 : 문서의 1% 미만으로 나타나는 단어 무시\n",
    "                # min_df = 10 : 문서에 10개 미만으로 나타나는 단어 무시\n",
    "                # max_df = 0.80 : 문서의 80% 이상에 나타나는 단어 무시\n",
    "                # max_df = 10 : 10개 이상의 문서에 나타나는 단어 무시\n",
    ")\n",
    " \n",
    "# 문장에서 노출되는 feature(특징이 될만한 단어) 수를 합한 Document Term Matrix(이하 DTM) 을 리턴한다\n",
    "X = vectorize.fit_transform(rawdata)\n",
    " \n",
    "print(\n",
    "    'fit_transform, (sentence {}, feature {})'.format(X.shape[0], X.shape[1])\n",
    ")\n",
    "# fit_transform, (sentence 5, feature 7)\n",
    " \n",
    "print(type(X))\n",
    "# <class 'scipy.sparse.csr.csr_matrix'>\n",
    " \n",
    "print(X.toarray())\n",
    " \n",
    "# [[0, 1, 2, 0, 0, 0, 1],\n",
    "# [0, 1, 1, 0, 0, 0, 2],\n",
    "# [1, 0, 0, 2, 1, 1, 0],\n",
    "# [1, 0, 0, 1, 0, 0, 0],\n",
    "# [0, 0, 0, 3, 1, 1, 0]]\n",
    " \n",
    "# 문장에서 뽑아낸 feature 들의 배열\n",
    "features = vectorize.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
