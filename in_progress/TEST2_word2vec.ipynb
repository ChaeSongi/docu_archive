{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from konlpy.utils import pprint\n",
    "from konlpy.corpus import kobill\n",
    "from konlpy.tag import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tv</th>\n",
       "      <th>title</th>\n",
       "      <th>epi</th>\n",
       "      <th>dates</th>\n",
       "      <th>links</th>\n",
       "      <th>synop</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ebs</td>\n",
       "      <td>다큐프라임</td>\n",
       "      <td>비밀의 땅 파미르  - 2부 비밀의 땅, 숨겨진 강</td>\n",
       "      <td>2018.09.11</td>\n",
       "      <td>http://www.ebs.co.kr/tv/show?courseId=BP0PAPB0...</td>\n",
       "      <td>아시아의 고산지대인 파미르, 텐샨,히말라야 의 자연과 사람들의 삶을 알아본다. 중앙...</td>\n",
       "      <td>늑대, 텐샨, 빙하, 고원, 저지대, 고지대, 사람들, 파미르, 야생동물</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ebs</td>\n",
       "      <td>다큐프라임</td>\n",
       "      <td>비밀의 땅 파미르 - 1부 세계의 지붕</td>\n",
       "      <td>2018.09.10</td>\n",
       "      <td>http://www.ebs.co.kr/tv/show?courseId=BP0PAPB0...</td>\n",
       "      <td>세계의 지붕 파미르에 서식하는 다양한 야생동물들의 생태소개 1부에서는 파미르에 서식...</td>\n",
       "      <td>늑대, 텐샨, 빙하, 고원, 저지대, 고지대, 사람들, 파미르, 야생동물</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    tv  title                           epi       dates  \\\n",
       "0  ebs  다큐프라임  비밀의 땅 파미르  - 2부 비밀의 땅, 숨겨진 강  2018.09.11   \n",
       "1  ebs  다큐프라임         비밀의 땅 파미르 - 1부 세계의 지붕  2018.09.10   \n",
       "\n",
       "                                               links  \\\n",
       "0  http://www.ebs.co.kr/tv/show?courseId=BP0PAPB0...   \n",
       "1  http://www.ebs.co.kr/tv/show?courseId=BP0PAPB0...   \n",
       "\n",
       "                                               synop  \\\n",
       "0  아시아의 고산지대인 파미르, 텐샨,히말라야 의 자연과 사람들의 삶을 알아본다. 중앙...   \n",
       "1  세계의 지붕 파미르에 서식하는 다양한 야생동물들의 생태소개 1부에서는 파미르에 서식...   \n",
       "\n",
       "                                        tag  \n",
       "0  늑대, 텐샨, 빙하, 고원, 저지대, 고지대, 사람들, 파미르, 야생동물  \n",
       "1  늑대, 텐샨, 빙하, 고원, 저지대, 고지대, 사람들, 파미르, 야생동물  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docu=pd.read_csv('tag_eb_songi_ver3.csv',encoding='cp949')\n",
    "docu.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "docu_synop = docu['synop']\n",
    "docu_tag = docu['tag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "han = Hannanum()\n",
    "kkma = Kkma()\n",
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 형태소 분석기 (Kkma, hannanum, Okt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### morphs : 형태소 추출 nouns : 명사 추출 pos : 품사 부착(pos tagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_noun = kkma.nouns(docu_synop[0])\n",
    "k_noun\n",
    "len(k_noun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_noun = hannanum.nouns(docu_synop[0])\n",
    "len(h_noun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o_noun = okt.nouns(docu_synop[0])\n",
    "len(o_noun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "267"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''han.morphs(docu_synop[0])\n",
    "len(han.morphs(docu_synop[0]))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hannanum이 의미있는 명사 가장 잘 추출 (test1개로 부정확할 가능성 매우 높음..)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = h_noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec 모델 학습에 로그를 찍을 수 있도록 합니다.\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)s : %(levelname)s : %(message)s', \n",
    "    level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-09-24 15:45:39,282 : INFO : collecting all words and their counts\n",
      "2018-09-24 15:45:39,283 : WARNING : Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n",
      "2018-09-24 15:45:39,284 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-09-24 15:45:39,286 : INFO : collected 103 word types from a corpus of 258 raw words and 89 sentences\n",
      "2018-09-24 15:45:39,287 : INFO : Loading a fresh vocabulary\n",
      "2018-09-24 15:45:39,289 : INFO : effective_min_count=1 retains 103 unique words (100% of original 103, drops 0)\n",
      "2018-09-24 15:45:39,290 : INFO : effective_min_count=1 leaves 258 word corpus (100% of original 258, drops 0)\n",
      "2018-09-24 15:45:39,292 : INFO : deleting the raw counts dictionary of 103 items\n",
      "2018-09-24 15:45:39,294 : INFO : sample=0.001 downsamples 103 most-common words\n",
      "2018-09-24 15:45:39,295 : INFO : downsampling leaves estimated 103 word corpus (40.3% of prior 258)\n",
      "2018-09-24 15:45:39,296 : INFO : estimated required memory for 103 words and 100 dimensions: 133900 bytes\n",
      "2018-09-24 15:45:39,297 : INFO : resetting layer weights\n",
      "2018-09-24 15:45:39,309 : INFO : training model with 3 workers on 103 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-09-24 15:45:39,320 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-09-24 15:45:39,321 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-09-24 15:45:39,322 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-09-24 15:45:39,323 : INFO : EPOCH - 1 : training on 258 raw words (99 effective words) took 0.0s, 23898 effective words/s\n",
      "2018-09-24 15:45:39,343 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-09-24 15:45:39,345 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-09-24 15:45:39,346 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-09-24 15:45:39,348 : INFO : EPOCH - 2 : training on 258 raw words (118 effective words) took 0.0s, 18641 effective words/s\n",
      "2018-09-24 15:45:39,359 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-09-24 15:45:39,361 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-09-24 15:45:39,362 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-09-24 15:45:39,363 : INFO : EPOCH - 3 : training on 258 raw words (111 effective words) took 0.0s, 23671 effective words/s\n",
      "2018-09-24 15:45:39,370 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-09-24 15:45:39,371 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-09-24 15:45:39,372 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-09-24 15:45:39,373 : INFO : EPOCH - 4 : training on 258 raw words (105 effective words) took 0.0s, 23161 effective words/s\n",
      "2018-09-24 15:45:39,378 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-09-24 15:45:39,379 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-09-24 15:45:39,380 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-09-24 15:45:39,381 : INFO : EPOCH - 5 : training on 258 raw words (108 effective words) took 0.0s, 32076 effective words/s\n",
      "2018-09-24 15:45:39,382 : INFO : training on a 1290 raw words (541 effective words) took 0.1s, 7470 effective words/s\n",
      "2018-09-24 15:45:39,383 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x2462ffc0b00>"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 초기화 및 모델 학습\n",
    "from gensim.models import word2vec\n",
    "\n",
    "# 모델 학습\n",
    "model = word2vec.Word2Vec(token, min_count=1)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-09-24 15:46:05,321 : INFO : saving Word2Vec object under 1minwords, separately None\n",
      "2018-09-24 15:46:05,322 : INFO : not storing attribute vectors_norm\n",
      "2018-09-24 15:46:05,325 : INFO : not storing attribute cum_table\n",
      "2018-09-24 15:46:05,340 : INFO : saved 1minwords\n"
     ]
    }
   ],
   "source": [
    "# 모델 이름을 지정하고 저장한다.\n",
    "model_name = '1minwords'\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어 사전 수\n",
    "len(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-09-24 15:47:04,416 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"word '중앙아시아' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-126-3d0d73cbb390>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'중앙아시아'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[1;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[0;32m    529\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 531\u001b[1;33m                 \u001b[0mmean\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_norm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    532\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m                     \u001b[0mall_words\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[1;34m(self, word, use_norm)\u001b[0m\n\u001b[0;32m    450\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 452\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"word '중앙아시아' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "model.wv.most_similar('중앙아시아')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenized_contents' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-111-f85a3cd5925c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# size = 벡터 차원 window = 주변단어 앞뒤 개수 min_count = 코퍼스 내 출현 빈도 iter = 반복 학습 횟수\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0membedding_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenized_contents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'tokenized_contents' is not defined"
     ]
    }
   ],
   "source": [
    "# Word2Vec embedding\n",
    "# size = 벡터 차원 window = 주변단어 앞뒤 개수 min_count = 코퍼스 내 출현 빈도 iter = 반복 학습 횟수 \n",
    "from gensim.models import Word2Vec\n",
    "embedding_model = Word2Vec(tokenized_contents, size=20, window = 2, min_count=10, workers=3, iter=20, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
