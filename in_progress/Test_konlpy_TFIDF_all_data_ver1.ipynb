{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "유사도 분석을 위한 3x3 matrix를 만들었습니다.\n",
      "[[1.         0.62111188 0.2554899 ]\n",
      " [0.62111188 1.         0.51770886]\n",
      " [0.2554899  0.51770886 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "mydoclist = ['find what you love', 'do what you love', \"don't do what you hate\"]\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=1)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(mydoclist)\n",
    "\n",
    "documnet_distance = (tfidf_matrix * tfidf_matrix.T)\n",
    "\n",
    "print('유사도 분석을 위한 ' + str(documnet_distance.get_shape()[0]) + 'x' + str(documnet_distance.get_shape()[1]) + ' matrix를 만들었습니다.')\n",
    "print(documnet_distance.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "유사도 분석을 위한 3x3 matrix를 만들었습니다.\n",
      "[[1.       0.208199 0.      ]\n",
      " [0.208199 1.       0.208199]\n",
      " [0.       0.208199 1.      ]]\n"
     ]
    }
   ],
   "source": [
    "mydoclist2 = ['영희가 좋아하는 사람은 철수다.', '철수를 영희가 좋아한다.', '영희는 철수를 좋아하고 있다.']\n",
    "\n",
    "tfidf_vectorizer2 = TfidfVectorizer(min_df=1)\n",
    "tfidf_matrix2 = tfidf_vectorizer2.fit_transform(mydoclist2)\n",
    "\n",
    "documnet_distance2 = (tfidf_matrix2 * tfidf_matrix2.T)\n",
    "\n",
    "print('유사도 분석을 위한 ' + str(documnet_distance2.get_shape()[0]) + 'x' + str(documnet_distance2.get_shape()[1]) + ' matrix를 만들었습니다.')\n",
    "print(documnet_distance2.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv('new_all_data_ver2.csv', encoding='cp949')\n",
    "del metadata['synop']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsynop = metadata['n_synop']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = nsynop[1890]\n",
    "s2 = nsynop[1868]\n",
    "s3 = nsynop[1550]\n",
    "s4 = nsynop[1683]\n",
    "s5 = nsynop[630]\n",
    "s6 = nsynop[267]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''s1 = '새벽 4시 일 능률 이유 비밀 당신 속 시계 현대인 병 생체시계 리듬 국내외 수 전문가들 공통적 추천 치료법 햇빛 잠 프로그램 적정 햇빛 충분한 수면 고장 생체시계 회복시키 방법 소개 누구 한번쯤 장거리 비행 후 시차 피로 방법 낮밤 생활 주야간 교대근무자들 건강 숙면 수 방법 당신 속 시간 몇 시입니까우리 몸 혁명 생체시계 이야기 수억 년 속 원초적 리듬 생체시계 말 시간대별 최고 행동 최악 행동들 이 이용 최첨단 의학적 접근 시간치료학 나 거 시간 개념이었죠내 5분 생각 때 2분 생체시계 탐험 동굴학자 미셸 시프레 1962년 자신 생체시계 탐험 300피트 아래 동굴 햇빛 차단 환경 두 달 프랑스 동굴학자 미셸 시프레 시계 시간 수 외부요소 제거 환경 그 속 시간 그 증명 생체시계 존재 이후 계속 두 번 실험 그 발견 생체시계 비밀 무엇 사람 동굴 속 생활 그 햇볕 경우 생체 리듬 그 유전적 작동 것 저널 시간생물학 인터내셔널 편집장 마법 생체시계 저 마이클 스몰렌스키 다큐프라 생체시계 비밀 인체 신비 인간 생존 비밀 생체시계 작동 메커니즘 과학적 우리 몸 최고 시간 최악 시간 생체시계 현대의학 접목 시간치료학 소개 우리 사회 아침형 인간 저녁형 인간 오해 진실 현대인 다양한 병 증상들 생체시계 관점 분석 프로그램 신체적 정신적 건강 행복 삶 영위 현대 들 생체시계 이해 기반 실제 우리 실생활 적용 수 유익 정보 제공 현대 사회 소홀 우리 몸 건강 수 숨 비법 공개 2부 못드 밤새벽 4시 일 능률 이유그 비밀 당신 속 시계 있다수면욕구 억제 수 생물 인간 밖 거예요 개 리 인간 잠 스트레스 호르몬 분비 잠 억제 인간 수면 사치 생각 바 의 저 러셀 포스터 수면호르몬 멜라토닌 분비 밤 잠 자 최적 시간 때문 사람들 밤 최악 시간 수 컨디션 말 수 때 텔레비전 나 프로 정도 방송이 대부분 것 보긴 집중 안보 그런가 새벽 1시 25분 서울 한 매점 주 김OO 수 밤낮 24시간 사회 속 사람들 그 생체시계 낮 밤 시간 24시간 동안 다양한 시간대 다양한 사람 이야기 그 목소 리 시간 몸 요구 귀 24시간 사회 생체시계 위험 당신 생체시계 두가지 열쇠 햇빛 전문가들 말 광합성 숙면 건강법 과거 동굴생활 원시 사냥 잠 생활 생활 밤 숙면 수 최상 리듬 제공 에디슨 전구 발명 후 24시간 빛 수 완전 어둠 거리 네온사인 불빛 집안 형광등 자기 전 시청 컴 퓨터 모니터 24시간 사회 우리 몸 리듬 파괴 나이 부족한 수 양 때문 혼란 우리 생체시 계 생체시계 작동 우리'\n",
    "s2 = '작심 1 시간꾸준 것 힘 시간 마법 1 시간 법칙노력 당신 변화시키는가매년 우리 작심삼일한다 작심삼일하 사람 사람 차 무엇일까전문가들은 분야 선천적 재능 간 세계적 전문가 1 시간 연습 필요 1 시간 법칙 그것 작심삼일하 사람 정반대 1 시간 노력파 그들 무언가 힘 무엇 누구 1 시간 연습 전문가 수 것일까같 1 시간 노력 폭발적 실력 사람 사람 차 무엇일까1 시간 주인공들 1 시간 속 비밀 캐롤 드웩 스탠퍼드대 심리학 교수 그것 1 시간 투입 것 1 시간동안 익숙해서 자신 요령 수 컴포트 매진 것 1 시간동안 체계적 자신 약점 파악 1 시간동안 장애물 마주 거 노력파 아이돌 1 시간 분석 남짓 한 곡 노래 퍼포먼스 시절 치열 무대 이면 시간들 대부분 국내 아이돌 1 시간 연습 그 성공 데 기획사 기획력 타이밍 등 다양한 외부 요 들 영향 중요 것 시간 노력 발전 사람 사람 것 차 무엇일까교육·사회 심리학자 뇌신경과학자 다양한 이론 속 그 연습 속 비밀 조 바조키스 신경과학자 연습 때 우리 머리 속 인터넷 두뇌 안 작용 수 연습 비밀 우리 뇌 속 거 ” 1 시간 법칙 을 증명 도전그렇다 누구 1 시간 투자하 전문가 수 것 여기 의문 사람 상업 사진작가 맥롤린 30세 일 프로골퍼 결심 전 세계 6천 만명 아마추어 골퍼 투 수 200명 불과 정도 골프 세계 분야 경지 분야 2010년 4월 15일 2시간 연습 시작 하루 6시간 주 6회 연습 1 시간 규칙 지금 약 5000 시간 훈련 맥롤린 34세 아마추어 골퍼 한번 경험 뭔가 1 시간 법칙 가설 한번 시험 일 몰입 가능성 사람들 것 맥롤린 포 사람 것 그 때 테니스 훈련 적 고등학교 시절 마라톤 1년 정도 한 후 신문사 사진기자 1년 후 우리 주변 작심삼일하 사람 그 4년 하나 목표 것 절반 성공 프로골퍼 것 것 자기 자신 것 성공 실패 두 번 문제 수 그 생전 처음 뭔가 시도 생각 때 세계 시간 마법 난생 경험 것 시간 당신 재능 당신그리 나이 시작 당신 메시지50대 몸짱 외국어 도전한 의사 공고 출신 미술해설가 그 모습 작심삼일하 사람 수 넘사벽 수 사차원 벽 주인공 그들 결국 우리 바 출발점 시작 작심삼일하 우리 그들 차 시도 노력 안했냐 차이 뿐우리 성공 잘못된 신화 것 재능 사람들만 정상 재능 노력 이야기 수 요소 재능 노력 첫걸음 마음 몸 나서라당신 정기적 일 당신 자신 (아리스토텔레스) 화 무대 이면 모습 엑소 카이 오디션 영상 최초 공개유노윤호 태민 카이 연습생 시절 영상 공개자신 시절 별명 음치 샤 태민 자신 한계 극복했을까서로에 선생 경쟁자 연습생 시절 절친 카 태민그들 유년기 대부분 연습 추억 데뷔 친구 카이 솔직한 심정 국내 최고 춤꾼 중 한 명 엑소 카이 연습방법 등 무대 수 그 진솔 모습 공개'\n",
    "s3 = '잠 대한민국2006년 15 명 2012년 35 7천 명 이상 급증 수면장애 환자 불면증 수 무호흡증 코골 기면병 등 이상 남 일 나 이야기 생체시계 인간 잠 반 잠 전쟁 벌 사람들 최신 연구결과 잠 비밀 불규칙 잠 당신 건강 위협 우리 건강 삶 잠 영향'\n",
    "s4 = '나라 행복 은둔 땅 부탄물질문명 우리 편리함 편리함 현대인 나 삶 속도 자본주의세계 팽창 결과 우리 과거 윤택 삶 영위 대가 것 지불 뉴욕 타임스 작년 10월4 행복한 왕국 행복 기준 제목 부탄 소개 부탄 세계 국내총생산(GDP) 확대 추구 상황 33년 부 분배 문화 전통 유지 환경보호 이상적 정책 고수 나라 1972년 17세 부탄 제4대 왕위 지그메 싱예 왕추크 국왕(현 51세 이 집약 국민 행복(GNH 개념 제시 요약 부탄 문화적 전통 유 친환경적 노력 지속 투명 책 정부 운영 기본 것 부탄 전 세계 부 축적 일직선 지향 정책들 역행 국토개발 관광객 수입 증대 노력 등 여기 가난 공동체적 삶 부탄인 문명 진보라 우리 사람 사람 사람 동물 사람 자연 관계 앞 우리 것들 것 감사 줄 마음 여유 상대방 배려… 부탄 우리 것 나라 우리 소중 것들 간직 인간 진정한 행복 얼굴 무엇 프로그램 물질문명 혜택 행복 수 여건 추적 현대문명 속 우리들 공간 제공토록 한다.1 지구상 나 부탄 들 정원 히말라 히말라야 품속 은둔 왕국 부탄 자신 부탄정부 자신들 전통문화 자연환경 보존 1 명 관광객 제한 관광객들 부탄 여행 약 230달러 지불 타인 손 생경 곳 발 2 도착 것 것 차 우리 곳 도착 수 안 단번 수 장소 일체 데 시간 우리 조금씩 세계 실망 것 쥐 빈대 이 벼룩 동네 개 우리 밤잠 방해 것이므로… 우리 실망 그것 수 의심 것 우리 것 것 아침 눈 때 우리 그곳 우리 자신 어디 시작 것 그전 우리 불편 낮 무엇 것이다.3 통 무엇 정도 사람들 가방 안 대 빈병 자식들 과 봉지 세상 듯 웃음 지 수 부탄인 눈 한 부처 수 부탄 들 행복 수 인생 화두 정도 충분 가난 행복 부탄 들 믿음 4 몽갈 마을 과거 시간 서양 사람 개인 경력 때 부탄사람들 공동 가치 중요시 그 법 옛날 필요 과거 풍경 과거 방식들 눈앞 시간 때문 표지판 광고판 대신 언덕 논밭 농가 하늘 곳 곳 견고한 공동체 생활 발견 5 자연 배 만족마부들 것 산 산 마부들 이 진정한 스승 자연 마부들 시간 몇 시 몇 분 어디 때 들 때 등 설명 아버지 다섯 마리 말 재산 전부 마부 미소 만족 단어 된다.6 야크텐트 아이들 해발 3000m 이상 고원 포브지카 골짜기 근처 일곱 가구 천막 이 것 생존 최소한 것들 텐트 안 가난 대 웃음소리 물질적 풍족 아무 가난 긴밀한 가족적 공동체적 자연 일치 삶 속 평화 충만한 생 우리 무엇 행복 것 문명 것 경쟁 파괴 아이 눈동자 속 진정 것 우리 시간 시간 속 천진한 맑음'\n",
    "s5 = '인간 알고리즘 기계 인간 기계 인간 무엇 전세계 뇌과학자들 인간 이야기 첨단 시대 우리 인간 집중 과학 발전 것 가능 우리 삶 윤택 뿐 인류 난제 해결 인간 낙관 것 변화 혼란 적응력 필요 것 인간 존재 등장 인류중심적 사고 균열 지 이 분명 인류 타격 수 의미 우리 고민 것 기술 인간 운명 미래 기술 탐구 인간 질문 인간 인간답 인간 영역 기술 등장 인간 경계 모호 이것 인간 단어 정의 필요 이유 인간다운 것 무엇 기술 구현 인간 무엇 인간 무엇 위 답 우리 과학기술 혁명 담대 것 로봇공학자 데니스홍 질문 시대 천재 과학자 로봇공학자 미국 대학 데니스홍 교수 세계 정상 연구 현장 위 질문 기술 발전 이면 인간 화두 제시 제2부 3월 6일(화) 밤 9시 50분 인간 기계인가인간 알고리즘 기계 수 인간 생각 마음 신비로운 영역 그것 인간 것 정의 능력 인간만 고유 영역 중요 기술 구현 수 영역 것 기계 인간 본질적 구분 때문 이 인간 존재이유 우리 생각 고유성 하나씩 우리 한번 인간 본질 생각 인간 기계 바 존재 것 최첨단 뇌과학 인간 낭만적 해석 해부 구글 딥마인드 개발 알파고(AlphaGo) 기존 인공지능 알고리즘 압도적 수 이유 인간 뇌 모방 때문 기계 비약적 발전 수 단 인간 전 세계 인간 뇌 한창 인간 알고리즘 뿐 로봇공학자 데니스홍은 인간 기계 질문 전세계 뇌과학자들 인간 이야기 뇌과학자 장동선 박사 인간 자유의지 지 실험 의심 인간 기계 인간 무엇 프로그램 뇌과학 연구 최전선 진행 인간 작동 원리 보고 탐험 낭만적 해석 우리 모습 확인'\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "유사도 분석을 위한 6x6 matrix를 만들었습니다.\n",
      "[[1.         0.18501764 0.18393711 0.11980341 0.12146953 0.0919242 ]\n",
      " [0.18501764 1.         0.11468878 0.22580827 0.07359285 0.19786429]\n",
      " [0.18393711 0.11468878 1.         0.15917905 0.14386143 0.06828317]\n",
      " [0.11980341 0.22580827 0.15917905 1.         0.18398039 0.12478887]\n",
      " [0.12146953 0.07359285 0.14386143 0.18398039 1.         0.08349429]\n",
      " [0.0919242  0.19786429 0.06828317 0.12478887 0.08349429 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "mydoclist2 = [s1, s2, s3, s4, s5, s6]\n",
    "tfidf_vectorizer2 = TfidfVectorizer(min_df=1)\n",
    "tfidf_matrix2 = tfidf_vectorizer2.fit_transform(mydoclist2)\n",
    "\n",
    "documnet_distance2 = (tfidf_matrix2 * tfidf_matrix2.T)\n",
    "\n",
    "print('유사도 분석을 위한 ' + str(documnet_distance2.get_shape()[0]) + 'x' + str(documnet_distance2.get_shape()[1]) + ' matrix를 만들었습니다.')\n",
    "print(documnet_distance2.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = nsynop[1888]\n",
    "s2 = nsynop[1889]\n",
    "s3 = nsynop[300]\n",
    "s4 = nsynop[1752]\n",
    "s5 = nsynop[1193]\n",
    "s6 = nsynop[1399]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "유사도 분석을 위한 6x6 matrix를 만들었습니다.\n",
      "[[1.         0.07251422 0.0624878  0.05063329 0.07318807 0.09537162]\n",
      " [0.07251422 1.         0.07182435 0.02219147 0.05327709 0.04054775]\n",
      " [0.0624878  0.07182435 1.         0.02619695 0.04691496 0.03543075]\n",
      " [0.05063329 0.02219147 0.02619695 1.         0.04066674 0.09538822]\n",
      " [0.07318807 0.05327709 0.04691496 0.04066674 1.         0.0610507 ]\n",
      " [0.09537162 0.04054775 0.03543075 0.09538822 0.0610507  1.        ]]\n"
     ]
    }
   ],
   "source": [
    "mydoclist2 = [s1, s2, s3, s4, s5, s6]\n",
    "tfidf_vectorizer2 = TfidfVectorizer(min_df=1)\n",
    "tfidf_matrix2 = tfidf_vectorizer2.fit_transform(mydoclist2)\n",
    "\n",
    "documnet_distance2 = (tfidf_matrix2 * tfidf_matrix2.T)\n",
    "\n",
    "print('유사도 분석을 위한 ' + str(documnet_distance2.get_shape()[0]) + 'x' + str(documnet_distance2.get_shape()[1]) + ' matrix를 만들었습니다.')\n",
    "print(documnet_distance2.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = nsynop[1881]\n",
    "s2 = nsynop[47]\n",
    "s3 = nsynop[48]\n",
    "s4 = nsynop[1494]\n",
    "s5 = nsynop[1649]\n",
    "s6 = nsynop[1549]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "유사도 분석을 위한 6x6 matrix를 만들었습니다.\n",
      "[[1.         0.15568459 0.14506764 0.12549817 0.10479903 0.1047492 ]\n",
      " [0.15568459 1.         0.32899437 0.15213165 0.16761156 0.19175106]\n",
      " [0.14506764 0.32899437 1.         0.11796181 0.15617299 0.24560961]\n",
      " [0.12549817 0.15213165 0.11796181 1.         0.0837394  0.12043202]\n",
      " [0.10479903 0.16761156 0.15617299 0.0837394  1.         0.20606375]\n",
      " [0.1047492  0.19175106 0.24560961 0.12043202 0.20606375 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "mydoclist2 = [s1, s2, s3, s4, s5, s6]\n",
    "tfidf_vectorizer2 = TfidfVectorizer(min_df=1)\n",
    "tfidf_matrix2 = tfidf_vectorizer2.fit_transform(mydoclist2)\n",
    "\n",
    "documnet_distance2 = (tfidf_matrix2 * tfidf_matrix2.T)\n",
    "\n",
    "print('유사도 분석을 위한 ' + str(documnet_distance2.get_shape()[0]) + 'x' + str(documnet_distance2.get_shape()[1]) + ' matrix를 만들었습니다.')\n",
    "print(documnet_distance2.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "mydoclist2 = []\n",
    "\n",
    "\n",
    "for i in nsynop:\n",
    "    mydoclist2.append(i)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'8주간 기적 제2부부부 관계 비밀 부부 사랑 전쟁 솔직대담한 이야기 8주간 기적 부부 일평생 격렬 사랑 때론 일생 동반자 순식간 적 부부 평생 몇 고비 부부 만큼 노력 것 부부 결혼생활 다큐프라 8주간 기적 연령대 결혼기간 방식 부부갈등 문제 두 쌍 부부 출연 이 부부 문제 삶 고비들 무엇 대화 상담 치료 등 해소 과정 사랑 미움 행복 불행 것 진정한 부부 의미 무엇 2009년 시대 평범한 부부들 자화상 8주간 상담 부부갈등극복기 결혼 17년차 결혼 3년차 두 쌍 부부 부부갈 고민 8주간 상담 치료 상담 초 공간 자기 이야기 것 어색하기 부부 시간 부부 마음 문 시작 치료 배우자 들 것 생각 배우자 진실 부부 간 사랑 계기 설정 정답 실제 부부들 100 휴먼다큐멘터리 프로그램 부부 보고 개론 부부상담 이외 인위적 화해 설정 장치 시대 부부들 꾸밈없 삶 뿐 극 변화 8주 기간 동안 조금씩 변화 부부들 모습 등 부부들 기적 것 2부 부부 관계 비밀아이 남편 미래 수 ”부부관계 거부 가사분담 요구 아내 때문 힘 결혼 3년 차 30대 초반 부부 이야기전형적 맞벌이 부부 둘 가사분담 딸 육아문제 매일 거기 스킨십 거부 아내 부부관계 수 상황 부부 등 회사 스트레스 퇴근 후 집 남편 아내 예외 동등 가사 육아 분담 요구 돌 채 딸 새벽 분유 일 남편 일 결혼 3년차 신혼 단꿈 시 부부 삶 한데이혼 외 방법 생각 부부 8주 시간 특별 시간 동안 이 일 일어났을까그리 상담 남편 아내 진실 되는데8주 동안 서로 이해 방법 이 기적 시작'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydoclist2[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer2 = TfidfVectorizer(min_df=1)\n",
    "tfidf_matrix2 = tfidf_vectorizer2.fit_transform(mydoclist2)\n",
    "\n",
    "documnet_distance2 = (tfidf_matrix2 * tfidf_matrix2.T)\n",
    "\n",
    "print('유사도 분석을 위한 ' + str(documnet_distance2.get_shape()[0]) + 'x' + str(documnet_distance2.get_shape()[1]) + ' matrix를 만들었습니다.')\n",
    "print(documnet_distance2.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "np.nan is an invalid document, expected byte or unicode string.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-7e6220c12617>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mtfidf_vectorizer2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin_df\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mtfidf_matrix2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfidf_vectorizer2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmydoclist2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mdocumnet_distance2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtfidf_matrix2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtfidf_matrix2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1379\u001b[0m             \u001b[0mTf\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0midf\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mweighted\u001b[0m \u001b[0mdocument\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mterm\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1380\u001b[0m         \"\"\"\n\u001b[1;32m-> 1381\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1382\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1383\u001b[0m         \u001b[1;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m    867\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    868\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[1;32m--> 869\u001b[1;33m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[0;32m    870\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    871\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m    790\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 792\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    793\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    794\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(doc)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[1;32m--> 266\u001b[1;33m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[0;32m    267\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, doc)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m             raise ValueError(\"np.nan is an invalid document, expected byte or \"\n\u001b[0m\u001b[0;32m    120\u001b[0m                              \"unicode string.\")\n\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: np.nan is an invalid document, expected byte or unicode string."
     ]
    }
   ],
   "source": [
    "#mydoclist2 = [nsynop[0], nsynop[1], nsynop[2], nsynop[3], nsynop[4]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tfidf_vectorizer2 = TfidfVectorizer(min_df=3)\n",
    "tfidf_matrix2 = tfidf_vectorizer2.fit_transform(mydoclist2)\n",
    "\n",
    "documnet_distance2 = (tfidf_matrix2 * tfidf_matrix2.T)\n",
    "\n",
    "print('유사도 분석을 위한 ' + str(documnet_distance2.get_shape()[0]) + 'x' + str(documnet_distance2.get_shape()[1]) + ' matrix를 만들었습니다.')\n",
    "print(documnet_distance2.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Kkma\n",
    "from konlpy.utils import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "형태소 : ['영희', '와', '철수', '는', '백구', '를', '산책', '시키', '기', '위하', '어', '한강', '에', '가', '었', '다', '.', '한강', '에', '도착', '하', '여', '누렁이', '를', '만나', '었', '다', '.']\n",
      "명사 : ['영희', '철수', '백구', '산책', '한강', '도착', '누렁이']\n",
      "품사 : [('영희', 'NNP'), ('와', 'JKM'), ('철수', 'NNG'), ('는', 'JX'), ('백구', 'NNG'), ('를', 'JKO'), ('산책', 'NNG'), ('시키', 'XSV'), ('기', 'ETN'), ('위하', 'VV'), ('어', 'ECS'), ('한강', 'NNP'), ('에', 'JKM'), ('가', 'VV'), ('었', 'EPT'), ('다', 'EFN'), ('.', 'SF'), ('한강', 'NNP'), ('에', 'JKM'), ('도착', 'NNG'), ('하', 'XSV'), ('여', 'ECS'), ('누렁이', 'NNG'), ('를', 'JKO'), ('만나', 'VV'), ('었', 'EPT'), ('다', 'EFN'), ('.', 'SF')]\n"
     ]
    }
   ],
   "source": [
    "kkma = Kkma()\n",
    "sentence = u'영희와 철수는 백구를 산책시키기 위해 한강에 갔다. 한강에 도착하여 누렁이를 만났다.'\n",
    "print('형태소 : ' + str(kkma.morphs(sentence)))\n",
    "\n",
    "print('명사 : ' + str(kkma.nouns(sentence)))\n",
    "\n",
    "print('품사 : ' + str(kkma.pos(sentence)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['영희', '사랑', '강아지', '백구', '산책']\n",
      "['철수', '사랑', '소', '누렁이', '운동']\n",
      "['영희', '철수', '소', '강아지', '산책', '운동']\n",
      "doc1 : 영희 사랑 강아지 백구 산책 \n",
      "doc2 : 철수 사랑 소 누렁이 운동 \n",
      "doc3 : 영희 철수 소 강아지 산책 운동 \n"
     ]
    }
   ],
   "source": [
    "mydoclist = ['방랑식객 放浪食客 2. 올레길 ” 기획의도웰빙 푸드마 리 마크로비오틱 바 위한새 개념들 오염 먹거리들 속 주목 개념들 실상우리 전통적 어머니 밥상 속 제철 음식 땅 나 음식 조미 가공 최소화한 자연음식 이 여기 자연요리 임지호 두 가지 보태우리 음식 미래 제시 기존 재료 한정 재료 발굴 과입으로 눈 음식 을 몸 실천 것 그 이번 여름 곳 제주 제주 그 기억 속 요리 시작 곳이자바닷바람 만큼 기운 풀 다양한 바다산물들 곳.과연 제주도 바다 화산석 지대 식재료들은 것들일까 식재료 생명력 요리법 무엇 제주 올레길 중심 자연과그 땅 강 한 사람들 속 임지호 음식 사람 사람 따뜻한 사랑이며건강한 미래 약속 확인 주요 내용 발굴 식재료들버려진 숯가마 흙 바다 석회 비밀제주 해변 화산석 단단 소금 엉겨붙 석회 채취 그것 음식 재료 것 상상하기 사람 발길 한라산 기슭 오래전 숯가마들 그곳 불 숯가마 흙 임지호 식재료 바다석회 숯가마 흙 혈액순환 효능 고집 전통어법 어부 선물이방익(78) 할아버지 제주도 마지막 원땀 물고기 원땀 민물 썰물 이용 물고기 전통어법 사이 할아버지 자리돔잡이 그 배 서너 명 만 목선 바닷속 자리떼 그물 방식 할아버지 원땀 목선 싱싱 문어 자리돔은이제 임지호의 손끝 제주도 맛 바닷바람 올레길 위 풀들32가지 풍 방풍나물 비롯해제주 해안 일대 바닷바람 들 그간 우리 밥상 풀들이지만그 들 속 소금 양념 대체 만 미묘한 맛 효능 세계 터전 주변 둘러보아도온갖 들 선사 진귀 맛 효능 누리 채취 수 것 아토피 1위 오명 무 녹두 힘풍요로운 자연 속 제주 아토피 발병 1위 오명 삼나무 꽃가루 기후 속 아토피 발생률 것 뚜렷 치료약 고통 아이들 임지호 무 녹두 선택 몸 열 아토피 치료 기본적 원리를가까운 재료 조리법 해결 것 잡초 힘 미래 아이들임지호 제주도 발 것 열두 살 해.이후 그 굶주림 시간 잡초 단련됐고그 인생 요리 세계 그 쓸모 잡초 이용해아이들 잡초자장면 강 한 힘 아이들 자신들 미래 열어가길 마음 만들어가는잡초자장면 속 그 삶 이력 미래 음식 모티브.', '철수가 사랑하는 소 누렁이를 운동시키고 있다.', '영희와 철수는 소와 강아지를 산책 및 운동시키고 있다.']\n",
    "\n",
    "kkma = Kkma()\n",
    "\n",
    "doc_nouns_list=[]\n",
    "\n",
    "for doc in mydoclist:\n",
    "    nouns = kkma.nouns(doc) #nouns = 영희, 사랑, 강아지, 백구, 산책\n",
    "    doc_nouns = ''\n",
    "    print(nouns)\n",
    "\n",
    "    \n",
    "    for noun in nouns:\n",
    "        doc_nouns += noun + ' '   \n",
    "    doc_nouns_list.append(doc_nouns)\n",
    "    \n",
    "for i in range(0, 3):\n",
    "    print('doc' + str(i+1) + ' : ' + str(doc_nouns_list[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "유사도 분석을 위한 3x3 matrix를 만들었습니다.\n",
      "[[1.         0.19212486 0.56053185]\n",
      " [0.19212486 1.         0.4113055 ]\n",
      " [0.56053185 0.4113055  1.        ]]\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(min_df=1)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(doc_nouns_list)\n",
    "\n",
    "document_distance = (tfidf_matrix * tfidf_matrix.T)\n",
    "\n",
    "print('유사도 분석을 위한 ' + str(documnet_distance.get_shape()[0]) + 'x' + str(documnet_distance.get_shape()[1]) + ' matrix를 만들었습니다.')\n",
    "print(document_distance.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from konlpy.tag import Kkma, Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tv</th>\n",
       "      <th>title</th>\n",
       "      <th>epi</th>\n",
       "      <th>dates</th>\n",
       "      <th>links</th>\n",
       "      <th>synop</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ebs</td>\n",
       "      <td>다큐프라임</td>\n",
       "      <td>비밀의 땅 파미르  - 2부 비밀의 땅, 숨겨진 강</td>\n",
       "      <td>2018.09.11</td>\n",
       "      <td>http://www.ebs.co.kr/tv/show?courseId=BP0PAPB0...</td>\n",
       "      <td>아시아의 고산지대인 파미르, 텐샨,히말라야 의 자연과 사람들의 삶을 알아본다. 중앙...</td>\n",
       "      <td>늑대, 텐샨, 빙하, 고원, 저지대, 고지대, 사람들, 파미르, 야생동물</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ebs</td>\n",
       "      <td>다큐프라임</td>\n",
       "      <td>비밀의 땅 파미르 - 1부 세계의 지붕</td>\n",
       "      <td>2018.09.10</td>\n",
       "      <td>http://www.ebs.co.kr/tv/show?courseId=BP0PAPB0...</td>\n",
       "      <td>세계의 지붕 파미르에 서식하는 다양한 야생동물들의 생태소개 1부에서는 파미르에 서식...</td>\n",
       "      <td>늑대, 텐샨, 빙하, 고원, 저지대, 고지대, 사람들, 파미르, 야생동물</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ebs</td>\n",
       "      <td>다큐프라임</td>\n",
       "      <td>&lt;위대한 로마&gt; 제작노트 - 로마는 하루아침에 이루어지지 않았다</td>\n",
       "      <td>2018.09.05</td>\n",
       "      <td>http://www.ebs.co.kr/tv/show?courseId=BP0PAPB0...</td>\n",
       "      <td>위대한 로마의 촬영 기간은 총 6주로, 3주간 튀니지에서 재연 촬영을 했고 나머지...</td>\n",
       "      <td>로마, 폼페이</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ebs</td>\n",
       "      <td>다큐프라임</td>\n",
       "      <td>제국의 도시 - 폼페이</td>\n",
       "      <td>2018.09.04</td>\n",
       "      <td>http://www.ebs.co.kr/tv/show?courseId=BP0PAPB0...</td>\n",
       "      <td>폼페이를 첨단 그래픽과 3D 입체영상으로 완벽 복원한다.  콜로세움이 완성되어 가던...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ebs</td>\n",
       "      <td>다큐프라임</td>\n",
       "      <td>황제들의 정치무대 - 콜로세움</td>\n",
       "      <td>2018.09.03</td>\n",
       "      <td>http://www.ebs.co.kr/tv/show?courseId=BP0PAPB0...</td>\n",
       "      <td>로마 최대 원형경기장 콜로세움을 첨단 그래픽과 3D 입체영상으로 완벽 복원한다.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    tv  title                                  epi       dates  \\\n",
       "0  ebs  다큐프라임         비밀의 땅 파미르  - 2부 비밀의 땅, 숨겨진 강  2018.09.11   \n",
       "1  ebs  다큐프라임                비밀의 땅 파미르 - 1부 세계의 지붕  2018.09.10   \n",
       "2  ebs  다큐프라임  <위대한 로마> 제작노트 - 로마는 하루아침에 이루어지지 않았다  2018.09.05   \n",
       "3  ebs  다큐프라임                         제국의 도시 - 폼페이  2018.09.04   \n",
       "4  ebs  다큐프라임                     황제들의 정치무대 - 콜로세움  2018.09.03   \n",
       "\n",
       "                                               links  \\\n",
       "0  http://www.ebs.co.kr/tv/show?courseId=BP0PAPB0...   \n",
       "1  http://www.ebs.co.kr/tv/show?courseId=BP0PAPB0...   \n",
       "2  http://www.ebs.co.kr/tv/show?courseId=BP0PAPB0...   \n",
       "3  http://www.ebs.co.kr/tv/show?courseId=BP0PAPB0...   \n",
       "4  http://www.ebs.co.kr/tv/show?courseId=BP0PAPB0...   \n",
       "\n",
       "                                               synop  \\\n",
       "0  아시아의 고산지대인 파미르, 텐샨,히말라야 의 자연과 사람들의 삶을 알아본다. 중앙...   \n",
       "1  세계의 지붕 파미르에 서식하는 다양한 야생동물들의 생태소개 1부에서는 파미르에 서식...   \n",
       "2   위대한 로마의 촬영 기간은 총 6주로, 3주간 튀니지에서 재연 촬영을 했고 나머지...   \n",
       "3  폼페이를 첨단 그래픽과 3D 입체영상으로 완벽 복원한다.  콜로세움이 완성되어 가던...   \n",
       "4       로마 최대 원형경기장 콜로세움을 첨단 그래픽과 3D 입체영상으로 완벽 복원한다.   \n",
       "\n",
       "                                        tag  \n",
       "0  늑대, 텐샨, 빙하, 고원, 저지대, 고지대, 사람들, 파미르, 야생동물  \n",
       "1  늑대, 텐샨, 빙하, 고원, 저지대, 고지대, 사람들, 파미르, 야생동물  \n",
       "2                                   로마, 폼페이  \n",
       "3                                       NaN  \n",
       "4                                       NaN  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('tag_eb_songi_ver3.csv',encoding='cp949')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    아시아의 고산지대인 파미르, 텐샨,히말라야 의 자연과 사람들의 삶을 알아본다. 중앙...\n",
       "1    세계의 지붕 파미르에 서식하는 다양한 야생동물들의 생태소개 1부에서는 파미르에 서식...\n",
       "2     위대한 로마의 촬영 기간은 총 6주로, 3주간 튀니지에서 재연 촬영을 했고 나머지...\n",
       "3    폼페이를 첨단 그래픽과 3D 입체영상으로 완벽 복원한다.  콜로세움이 완성되어 가던...\n",
       "4         로마 최대 원형경기장 콜로세움을 첨단 그래픽과 3D 입체영상으로 완벽 복원한다.\n",
       "Name: synop, dtype: object"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence=df['synop']\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Kkma, Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['아시아', '고산', '고산지대', '지대', '파', '파미르', '미르', '터', '히', '히말라야', '말라야', '의', '자연', '사람', '삶', '중앙', '중앙아시아', '거대', '호수', '아랄해', '지금', '20', '20세기', '세기', '최악', '환경', '환경재앙', '재앙', '곳', '2', '2부', '부', '물줄기', '다리아', '다리아강', '강', '과', '시', '시르다리아강', '르', '을', '발원지', '여정', '주요', '에피소드', '파미르고원', '고원', '대륙', '대륙빙하', '빙하', '중', '최대', '규모', '페드첸', '페드첸코', '코', '물', '판즈강', '키스', '아프가니스탄', '국경', '강인', '수', '프', '프가니스', '니스', '모습', '습지', '습지대까지', '대까지', '식생', '변화', '기대어', '소개', '산맥', '고도', '풍경', '자랑', '초록', '산', '색색', '꽃', '시르다리', '다리', '자일로', '여름', '목지', '유르트', '양', '키', '키르', '유목', '유목민', '민', '야생', '야생동물', '동물']\n"
     ]
    }
   ],
   "source": [
    "print(kkma.nouns(sentence[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아시아 고산 고산지대 지대 파 파미르 미르 터 히 히말라야 말라야 의 자연 사람 삶 중앙 중앙아시아 거대 호수 아랄해 지금 20 20세기 세기 최악 환경 환경재앙 재앙 곳 2 2부 부 물줄기 다리아 다리아강 강 과 시 시르다리아강 르 을 발원지 여정 주요 에피소드 파미르고원 고원 대륙 대륙빙하 빙하 중 최대 규모 페드첸 페드첸코 코 물 판즈강 키스 아프가니스탄 국경 강인 수 프 프가니스 니스 모습 습지 습지대까지 대까지 식생 변화 기대어 소개 산맥 고도 풍경 자랑 초록 산 색색 꽃 시르다리 다리 자일로 여름 목지 유르트 양 키 키르 유목 유목민 민 야생 야생동물 동물\n"
     ]
    }
   ],
   "source": [
    "mydoclist_kkma=[]\n",
    "nouns = ' '.join(kkma.nouns(sentence[0]))\n",
    "mydoclist_kkma.append(nouns)\n",
    "print(nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['아시아의 고산지대인 파미르, 텐샨,히말라야 의 자연과 사람들의 삶을 알아본다. 중앙아시아의 거대한 호수 ‘아랄해’, 지금은 거의 말라버려 20세기 최악의 환경재앙이라 불리는 곳입니다. 2부는 이 아랄해로 들어오는 두 물줄기 ‘아무다리아강’과 ‘시르다리아강’을 따라 그 발원지를 찾아가는 여정입니다.   <주요 에피소드> 아무다리아강의 발원지, 파미르고원 대륙빙하 중 최대 규모인 파미르의 페드첸코 빙하에서부터 녹아 흐르는 물은 판즈강을 거쳐 아무다리아 강이 된다. 타지키스탄과 아프가니스탄의 국경을 이루는 강인 판즈강을 따라 가면 쉽게 볼 수 없는 아프가니스탄 사람들의 모습을 볼 수 있다. 또 빙하부터 습지대까지 식생의 변화와, 환경의 변화를 살펴보고 강에 기대어 살아가는 사람들의 삶을 소개한다.   시르다리아강의 발원지는 텐샨산맥 파미르 고원에 비해 고도가 낮은 텐샨산맥은 아름다운 풍경을 자랑한다. 초록으로 뒤덮인 산과 색색의 꽃들은 화려한 텐샨을 만든다. 시르다리야강의 발원지인 텐샨의 빙하는 어떤 모습인지 소개한다. 자일로(여름 초목지)에 유르트를 짓고 양을 치는 키르기 유목민, 텐샨산맥의 야생동물을 만난다.  ',\n",
       " '세계의 지붕 파미르에 서식하는 다양한 야생동물들의 생태소개 1부에서는 파미르에 서식하는 다양한 야생동물과 파미르의 지역생태를 소개합니다. 그중에서도 특히 최고 포식자인 늑대를 중심으로 거친 생존의 현장을 보여줍니다. 늑대 이외에도 눈표범, 마르코폴로 양, 아이벡스, 수염수리 등 다양한 동물들이 어떤 관계를 맺으며 황량한 파미르에서 살아가는지를 소개합니다.   <주요 에피소드> 늑대와 인간 : 늑대의 먹이는 야생동물이지만 겨울이 오면 가축을 공격한다. 양, 염소, 야크와 같은 가축을 키우며 유목생활을 하는 파미르인들에게는 두려움과 원망의 대상이다. 유목민과 늑대의 이야기를 통해 이들의 적대적인 공존관계를 보여주고자 한다.   세계 최초 최상위 포식자들의 만남 눈표범과 늑대가 하나의 먹이를 두고 신경전을 벌인다. 그 어디에서도 보기 어려운 진귀한 광경으로 생생한 야생의 숨결을 느낄 수 있다. ??',\n",
       " ' 위대한 로마의 촬영 기간은 총 6주로, 3주간 튀니지에서 재연 촬영을 했고 나머지 3주는 이탈리아 로마, 폼페이에서 건축물 촬영을 했다. 촬영 기간은 6주이지만 제작기간은 2년인 위대한 로마는 그 커다란 규모만큼이나, 크고 작은 사건 사고들이 일어나 순탄치만은 않은 제작이었다. 3D 제작의 전 과정과 제작진들의 고뇌, 노하우를 공개한다.',\n",
       " \"폼페이를 첨단 그래픽과 3D 입체영상으로 완벽 복원한다.  콜로세움이 완성되어 가던 서기79년, 인구 2만명의 휴양 도시였던 폼페이는 갑작스런 화산폭 발로 순식간에 역사 속으로 사라진다. 2천년전 인류가 겪은 비극의 역사가 아이러니하게 현 대 고고학의 소중한 유산으로 남은 곳이 폼페이다. 로마의 역사는 폼페이를 통해 '황제중심' 에서 '민중중심'으로 그 무게중심이 옮겨지게 된다. 2천년전 로마인의 삶이 어떻게 이루어졌는지, 그 실상을 간직하고 있는곳!   로마인의 삶이 얼마나 정교하고 발달된 문명위에서 이루어졌는지, 특히 그들의 삶에 깊게 뿌 리를 내리고 있는 공공성은 우리가 살고 있는 현대의 그것과 비교해도 전혀 모자람이 없다는 사실은 우리를 놀라게 할 것이다\",\n",
       " '로마 최대 원형경기장 콜로세움을 첨단 그래픽과 3D 입체영상으로 완벽 복원한다.']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#docu_list = [sentence[0], sentence[1], sentence[2], sentence[3], sentence[4]]\n",
    "#docu_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dayoi\\Anaconda3\\lib\\site-packages\\konlpy\\tag\\_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    }
   ],
   "source": [
    "twitter = Twitter()\n",
    "kkma = Kkma()\n",
    "\n",
    "mydoculist_kkma = []\n",
    "mydoculist_twitter = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.08781464 0.01276021 0.         0.02371196 1.\n",
      "  0.08781464 0.01276021 0.         0.02371196 1.         0.08781464\n",
      "  0.01276021 0.         0.02371196 1.         0.08781464 0.01276021\n",
      "  0.         0.02371196]\n",
      " [0.08781464 1.         0.         0.01110031 0.         0.08781464\n",
      "  1.         0.         0.01110031 0.         0.08781464 1.\n",
      "  0.         0.01110031 0.         0.08781464 1.         0.\n",
      "  0.01110031 0.        ]\n",
      " [0.01276021 0.         1.         0.02647692 0.02323996 0.01276021\n",
      "  0.         1.         0.02647692 0.02323996 0.01276021 0.\n",
      "  1.         0.02647692 0.02323996 0.01276021 0.         1.\n",
      "  0.02647692 0.02323996]\n",
      " [0.         0.01110031 0.02647692 1.         0.25901492 0.\n",
      "  0.01110031 0.02647692 1.         0.25901492 0.         0.01110031\n",
      "  0.02647692 1.         0.25901492 0.         0.01110031 0.02647692\n",
      "  1.         0.25901492]\n",
      " [0.02371196 0.         0.02323996 0.25901492 1.         0.02371196\n",
      "  0.         0.02323996 0.25901492 1.         0.02371196 0.\n",
      "  0.02323996 0.25901492 1.         0.02371196 0.         0.02323996\n",
      "  0.25901492 1.        ]\n",
      " [1.         0.08781464 0.01276021 0.         0.02371196 1.\n",
      "  0.08781464 0.01276021 0.         0.02371196 1.         0.08781464\n",
      "  0.01276021 0.         0.02371196 1.         0.08781464 0.01276021\n",
      "  0.         0.02371196]\n",
      " [0.08781464 1.         0.         0.01110031 0.         0.08781464\n",
      "  1.         0.         0.01110031 0.         0.08781464 1.\n",
      "  0.         0.01110031 0.         0.08781464 1.         0.\n",
      "  0.01110031 0.        ]\n",
      " [0.01276021 0.         1.         0.02647692 0.02323996 0.01276021\n",
      "  0.         1.         0.02647692 0.02323996 0.01276021 0.\n",
      "  1.         0.02647692 0.02323996 0.01276021 0.         1.\n",
      "  0.02647692 0.02323996]\n",
      " [0.         0.01110031 0.02647692 1.         0.25901492 0.\n",
      "  0.01110031 0.02647692 1.         0.25901492 0.         0.01110031\n",
      "  0.02647692 1.         0.25901492 0.         0.01110031 0.02647692\n",
      "  1.         0.25901492]\n",
      " [0.02371196 0.         0.02323996 0.25901492 1.         0.02371196\n",
      "  0.         0.02323996 0.25901492 1.         0.02371196 0.\n",
      "  0.02323996 0.25901492 1.         0.02371196 0.         0.02323996\n",
      "  0.25901492 1.        ]\n",
      " [1.         0.08781464 0.01276021 0.         0.02371196 1.\n",
      "  0.08781464 0.01276021 0.         0.02371196 1.         0.08781464\n",
      "  0.01276021 0.         0.02371196 1.         0.08781464 0.01276021\n",
      "  0.         0.02371196]\n",
      " [0.08781464 1.         0.         0.01110031 0.         0.08781464\n",
      "  1.         0.         0.01110031 0.         0.08781464 1.\n",
      "  0.         0.01110031 0.         0.08781464 1.         0.\n",
      "  0.01110031 0.        ]\n",
      " [0.01276021 0.         1.         0.02647692 0.02323996 0.01276021\n",
      "  0.         1.         0.02647692 0.02323996 0.01276021 0.\n",
      "  1.         0.02647692 0.02323996 0.01276021 0.         1.\n",
      "  0.02647692 0.02323996]\n",
      " [0.         0.01110031 0.02647692 1.         0.25901492 0.\n",
      "  0.01110031 0.02647692 1.         0.25901492 0.         0.01110031\n",
      "  0.02647692 1.         0.25901492 0.         0.01110031 0.02647692\n",
      "  1.         0.25901492]\n",
      " [0.02371196 0.         0.02323996 0.25901492 1.         0.02371196\n",
      "  0.         0.02323996 0.25901492 1.         0.02371196 0.\n",
      "  0.02323996 0.25901492 1.         0.02371196 0.         0.02323996\n",
      "  0.25901492 1.        ]\n",
      " [1.         0.08781464 0.01276021 0.         0.02371196 1.\n",
      "  0.08781464 0.01276021 0.         0.02371196 1.         0.08781464\n",
      "  0.01276021 0.         0.02371196 1.         0.08781464 0.01276021\n",
      "  0.         0.02371196]\n",
      " [0.08781464 1.         0.         0.01110031 0.         0.08781464\n",
      "  1.         0.         0.01110031 0.         0.08781464 1.\n",
      "  0.         0.01110031 0.         0.08781464 1.         0.\n",
      "  0.01110031 0.        ]\n",
      " [0.01276021 0.         1.         0.02647692 0.02323996 0.01276021\n",
      "  0.         1.         0.02647692 0.02323996 0.01276021 0.\n",
      "  1.         0.02647692 0.02323996 0.01276021 0.         1.\n",
      "  0.02647692 0.02323996]\n",
      " [0.         0.01110031 0.02647692 1.         0.25901492 0.\n",
      "  0.01110031 0.02647692 1.         0.25901492 0.         0.01110031\n",
      "  0.02647692 1.         0.25901492 0.         0.01110031 0.02647692\n",
      "  1.         0.25901492]\n",
      " [0.02371196 0.         0.02323996 0.25901492 1.         0.02371196\n",
      "  0.         0.02323996 0.25901492 1.         0.02371196 0.\n",
      "  0.02323996 0.25901492 1.         0.02371196 0.         0.02323996\n",
      "  0.25901492 1.        ]]\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for docu in docu_list:\n",
    "    kkma_nouns = ' '.join(kkma.nouns(docu))\n",
    "    twitter_nouns = ' '.join(twitter.nouns(docu))\n",
    "    mydoculist_kkma.append(kkma_nouns)\n",
    "    mydoculist_twitter.append(twitter.nouns)\n",
    "    \n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=1)\n",
    "tfidf_matrix_kkma = tfidf_vectorizer.fit_transform(mydoculist_kkma)\n",
    "#tfidf_matrix_twitter = tfidf_vectorizer.fit_transform(mydoculist_twitter)\n",
    "\n",
    "document_distances_kkma = (tfidf_matrix_kkma * tfidf_matrix_kkma.T)\n",
    "#document_distances_twitter = (tfidf_matrix_twitter * tfidf_matrix_twitter.T)\n",
    "\n",
    "\n",
    "print(document_distances_kkma.toarray())\n",
    "\n",
    "\n",
    "print(' ')\n",
    "\n",
    "#print(document_distances_twitter.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
